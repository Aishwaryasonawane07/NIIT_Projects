{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1379d79",
   "metadata": {},
   "source": [
    "# Task 1 :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aaca7696",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "en = spacy.load('en_core_web_sm')\n",
    "stop_words = en.Defaults.stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa2a1bfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First top 10 stop words : ['then', 'itself', 'may', 'really', 'mine', 'although', 'why', 'much', 'hereupon', 'for', 'being', 'we', 'please', 'my', 'rather']\n"
     ]
    }
   ],
   "source": [
    "print(\"First top 10 stop words :\", list(stop_words)[:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a85971b",
   "metadata": {},
   "source": [
    "# Task 2 \n",
    "Apply stemming and lemmatization on the tokens given below.\n",
    "which token gives better output, and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac7c673",
   "metadata": {},
   "source": [
    "### stemming :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e53d8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(language='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ddc41cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "Tokens = ['cries', 'this', 'lied', 'computing', 'organizing', 'matches']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff01aac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cries---> cri\n",
      "this---> this\n",
      "lied---> lie\n",
      "computing---> comput\n",
      "organizing---> organ\n",
      "matches---> match\n"
     ]
    }
   ],
   "source": [
    "for token in Tokens :\n",
    "    print(token + '--->', stemmer.stem(token))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3e4ec2",
   "metadata": {},
   "source": [
    "### lemmatization :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "916b045f",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = en('cries this lied computing organizing matches')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe66aafb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cries --> cry\n",
      "this --> this\n",
      "lied --> lie\n",
      "computing --> computing\n",
      "organizing --> organize\n",
      "matches --> match\n"
     ]
    }
   ],
   "source": [
    "for word in doc:\n",
    "    print(word.text, '-->', word.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7538a260",
   "metadata": {},
   "source": [
    "### Compare stemming and lemmatization :\n",
    "- The main difference between stemming and lemmatization is that stemming simply removes the suffixes from words, whereas lemmatization transforms words into their base forms using a dictionary of known words.\n",
    "- As we can see, stemming often results in shorter words that may not be actual words, while lemmatization results in actual words that are in the dictionary. In general, lemmatization is more accurate and useful for many natural language processing tasks. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b9ae47",
   "metadata": {},
   "source": [
    "# Task 3 :\n",
    "\n",
    "- Write the output after removal of stop words for the following text data.\n",
    "- a) Input text file scifiscripts_intro.txt\n",
    "- b) Five sentences of your choice using spacy \n",
    "- Print the result after the removal of stop words.\n",
    "- for the second part, take any five sentences as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b3ded4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"D:\\Data Science\\Course 12\\DS3_C2_S3_ScifiscriptsIntro_Data_Practice.txt\",'r').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf0d1c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_txt = [ ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9aeecc09",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = en(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d3118863",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "163"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6151e9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in doc :\n",
    "    if word.is_stop == False:\n",
    "        filter_txt.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "485c9130",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of stop words present in spacy english model :  326\n"
     ]
    }
   ],
   "source": [
    "print('Total number of stop words present in spacy english model : ', len(stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3f0d9bec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words excludind stop words 84\n"
     ]
    }
   ],
   "source": [
    "print('Total number of words excludind stop words', len(filter_txt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6b15f5",
   "metadata": {},
   "source": [
    "## b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0d8042e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2 = en('The main difference between stemming and lemmatization is that stemming simply removes the suffixes from words, whereas lemmatization transforms words into their base forms using a dictionary of known words.As we can see, stemming often results in shorter words that may not be actual words, while lemmatization results in actual words that are in the dictionary. In general, lemmatization is more accurate and useful for many natural language processing tasks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "11683681",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(doc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "42439416",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_txt2 = [ ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "85114cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in doc2 :\n",
    "    if word.is_stop == False:\n",
    "        filter_txt2.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5a6e64bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words excludind stop words 42\n"
     ]
    }
   ],
   "source": [
    "print('Total number of words excludind stop words', len(filter_txt2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61945f7",
   "metadata": {},
   "source": [
    "# Task 4 :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57dab3cd",
   "metadata": {},
   "source": [
    "For each word in the sentence given below, write the corresponding POS and tagg it with a description\n",
    "\n",
    "\"Bryan visited his friend for a while and then went home at 10 p.m\"\n",
    "\n",
    "Apply POS using spacy and check the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e309bad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = en(\"Bryan visited his friend for a while and then went home at 10 p.m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9d95e8f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bryan \t\t PROPN \t\t NNP\n",
      "visited \t\t VERB \t\t VBD\n",
      "his \t\t PRON \t\t PRP$\n",
      "friend \t\t NOUN \t\t NN\n",
      "for \t\t ADP \t\t IN\n",
      "a \t\t DET \t\t DT\n",
      "while \t\t NOUN \t\t NN\n",
      "and \t\t CCONJ \t\t CC\n",
      "then \t\t ADV \t\t RB\n",
      "went \t\t VERB \t\t VBD\n",
      "home \t\t ADV \t\t RB\n",
      "at \t\t ADP \t\t IN\n",
      "10 \t\t NUM \t\t CD\n",
      "p.m \t\t NOUN \t\t NN\n"
     ]
    }
   ],
   "source": [
    "for word in doc:\n",
    "    print(word.text, '\\t\\t', word.pos_, '\\t\\t', word.tag_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b5977c",
   "metadata": {},
   "source": [
    "# Task 5 :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cafc914e",
   "metadata": {},
   "outputs": [],
   "source": [
    "f3 = open(\"D:\\Data Science\\Course 12\\DS3_C2_S3_Random_Data_Practice.txt\",'r').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2ac4fb69",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = en(f3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3c9826a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PADUA \t\t VERB \t\t VBD\n",
      "HIGH \t\t ADJ \t\t JJ\n",
      "SCHOOL \t\t PROPN \t\t NNP\n",
      "- \t\t PUNCT \t\t HYPH\n",
      "DAY \t\t PROPN \t\t NNP\n",
      "\n",
      " \t\t SPACE \t\t _SP\n",
      "Revision \t\t PROPN \t\t NNP\n",
      "November \t\t PROPN \t\t NNP\n",
      "12 \t\t NUM \t\t CD\n",
      ", \t\t PUNCT \t\t ,\n",
      "1997 \t\t NUM \t\t CD\n",
      "\n",
      " \t\t SPACE \t\t _SP\n",
      "Total tokens: 12\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for word in doc:\n",
    "    print(word.text, '\\t\\t', word.pos_, '\\t\\t', word.tag_)\n",
    "    count += 1\n",
    "print(\"Total tokens:\", count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f5e08c",
   "metadata": {},
   "source": [
    "# Task 6 :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cbc2ba1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original stop word length :  326\n"
     ]
    }
   ],
   "source": [
    "print(\"Original stop word length : \", len(stop_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c09299c",
   "metadata": {},
   "source": [
    "### Add 5 words in stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5a005aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "en.Defaults.stop_words |= {\"HIGH\", \"SCHOOL\", \"Revision\",\"November\", \"friend\" }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "63840049",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length after addind 5 words : 331\n"
     ]
    }
   ],
   "source": [
    "print(\"Length after addind 5 words :\", len(stop_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028e5669",
   "metadata": {},
   "source": [
    "### Remove 4 stop words :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "92e74e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "en.Defaults.stop_words -= {\"HIGH\", \"SCHOOL\", \"Revision\",\"November\", \"friend\",\"always\", \"never\",\"between\",\"becomes\" }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "de45a859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length after Removing 4 words from original stop words : 322\n"
     ]
    }
   ],
   "source": [
    "print(\"Length after Removing 4 words from original stop words :\", len(stop_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d200aa77",
   "metadata": {},
   "source": [
    "# Task 7 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2cbf1e5",
   "metadata": {},
   "source": [
    "-  Apply tokenization.\n",
    "- Remove stop word from input .\n",
    "- Apply lemmatization .\n",
    "- Apply POS tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8e11d6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"D:\\Data Science\\Course 12\\DS3_C2_S3_Sample_Data_Practice.txt\",'r').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d0ca255b",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = en(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2bd853",
   "metadata": {},
   "source": [
    "### Apply tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d6994a76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PADUA\n",
      "HIGH\n",
      "SCHOOL\n",
      "-\n",
      "DAY\n",
      "\n",
      "\n",
      "Revision\n",
      "November\n",
      "12\n",
      ",\n",
      "1997\n",
      "\n",
      "\n",
      "I\n",
      "hope\n",
      "dinner\n",
      "'s\n",
      "ready\n",
      "because\n",
      "I\n",
      "only\n",
      "have\n",
      "ten\n",
      "minutes\n",
      "before\n",
      "Mrs.\n",
      "Johnson\n",
      "squirts\n",
      "out\n",
      "a\n",
      "screamer\n",
      ".\n",
      "\n",
      "\n",
      "He\n",
      "grabs\n",
      "the\n",
      "mail\n",
      "and\n",
      "rifles\n",
      "through\n",
      "it\n",
      ",\n",
      "as\n",
      "he\n",
      "bends\n",
      "down\n",
      "to\n",
      "kiss\n",
      "Sharon\n",
      "on\n",
      "the\n",
      "cheek\n",
      ".\n",
      "\n",
      "\n",
      "MICHAEL-\n",
      "C'm\n",
      "on\n",
      ".\n",
      "I\n",
      "'m\n",
      "supposed\n",
      "to\n",
      "give\n",
      "you\n",
      "the\n",
      "tour\n",
      ".\n",
      "They\n",
      "head\n",
      "out\n",
      "of\n",
      "the\n",
      "office\n",
      "\n",
      "\n",
      "MICHAEL\n",
      "(\n",
      "continuing)-\n",
      "So\n",
      "--\n",
      "which\n",
      "Dakota\n",
      "you\n",
      "from\n",
      "?\n",
      "\n",
      "          \n",
      "                                 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749334cf",
   "metadata": {},
   "source": [
    "### Remove stop word from input ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bbe0c64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "184a339e",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_sent = [ ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bb19db03",
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in doc:\n",
    "    if word.is_stop == False:\n",
    "        filter_sent.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "61f86ec6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PADUA,\n",
       " HIGH,\n",
       " SCHOOL,\n",
       " -,\n",
       " DAY,\n",
       " ,\n",
       " Revision,\n",
       " November,\n",
       " 12,\n",
       " ,,\n",
       " 1997,\n",
       " ,\n",
       " hope,\n",
       " dinner,\n",
       " ready,\n",
       " minutes,\n",
       " Mrs.,\n",
       " Johnson,\n",
       " squirts,\n",
       " screamer,\n",
       " .,\n",
       " ,\n",
       " grabs,\n",
       " mail,\n",
       " rifles,\n",
       " ,,\n",
       " bends,\n",
       " kiss,\n",
       " Sharon,\n",
       " cheek,\n",
       " .,\n",
       " ,\n",
       " MICHAEL-,\n",
       " C'm,\n",
       " .,\n",
       " supposed,\n",
       " tour,\n",
       " .,\n",
       " head,\n",
       " office,\n",
       " ,\n",
       " MICHAEL,\n",
       " (,\n",
       " continuing)-,\n",
       " --,\n",
       " Dakota,\n",
       " ?,\n",
       " \n",
       "           \n",
       "                                  ]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filter_sent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8379943",
   "metadata": {},
   "source": [
    "### Apply lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c2be374c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PADUA --> PADUA\n",
      "HIGH --> high\n",
      "SCHOOL --> SCHOOL\n",
      "- --> -\n",
      "DAY --> DAY\n",
      "\n",
      " --> \n",
      "\n",
      "Revision --> Revision\n",
      "November --> November\n",
      "12 --> 12\n",
      ", --> ,\n",
      "1997 --> 1997\n",
      "\n",
      " --> \n",
      "\n",
      "I --> I\n",
      "hope --> hope\n",
      "dinner --> dinner\n",
      "'s --> 's\n",
      "ready --> ready\n",
      "because --> because\n",
      "I --> I\n",
      "only --> only\n",
      "have --> have\n",
      "ten --> ten\n",
      "minutes --> minute\n",
      "before --> before\n",
      "Mrs. --> Mrs.\n",
      "Johnson --> Johnson\n",
      "squirts --> squirt\n",
      "out --> out\n",
      "a --> a\n",
      "screamer --> screamer\n",
      ". --> .\n",
      "\n",
      " --> \n",
      "\n",
      "He --> he\n",
      "grabs --> grab\n",
      "the --> the\n",
      "mail --> mail\n",
      "and --> and\n",
      "rifles --> rifle\n",
      "through --> through\n",
      "it --> it\n",
      ", --> ,\n",
      "as --> as\n",
      "he --> he\n",
      "bends --> bend\n",
      "down --> down\n",
      "to --> to\n",
      "kiss --> kiss\n",
      "Sharon --> Sharon\n",
      "on --> on\n",
      "the --> the\n",
      "cheek --> cheek\n",
      ". --> .\n",
      "\n",
      " --> \n",
      "\n",
      "MICHAEL- --> MICHAEL-\n",
      "C'm --> come\n",
      "on --> on\n",
      ". --> .\n",
      "I --> I\n",
      "'m --> be\n",
      "supposed --> suppose\n",
      "to --> to\n",
      "give --> give\n",
      "you --> you\n",
      "the --> the\n",
      "tour --> tour\n",
      ". --> .\n",
      "They --> they\n",
      "head --> head\n",
      "out --> out\n",
      "of --> of\n",
      "the --> the\n",
      "office --> office\n",
      "\n",
      " --> \n",
      "\n",
      "MICHAEL --> MICHAEL\n",
      "( --> (\n",
      "continuing)- --> continuing)-\n",
      "So --> so\n",
      "-- --> --\n",
      "which --> which\n",
      "Dakota --> Dakota\n",
      "you --> you\n",
      "from --> from\n",
      "? --> ?\n",
      "\n",
      "          \n",
      "                                 \n",
      " --> \n",
      "          \n",
      "                                 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for word in doc :\n",
    "    print(word.text, '-->',word.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a93ec4",
   "metadata": {},
   "source": [
    "### Apply POS tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fe1fcdeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PADUA \t\t VERB \t\t VBD\n",
      "HIGH \t\t ADJ \t\t JJ\n",
      "SCHOOL \t\t PROPN \t\t NNP\n",
      "- \t\t PUNCT \t\t HYPH\n",
      "DAY \t\t PROPN \t\t NNP\n",
      "\n",
      " \t\t SPACE \t\t _SP\n",
      "Revision \t\t PROPN \t\t NNP\n",
      "November \t\t PROPN \t\t NNP\n",
      "12 \t\t NUM \t\t CD\n",
      ", \t\t PUNCT \t\t ,\n",
      "1997 \t\t NUM \t\t CD\n",
      "\n",
      " \t\t SPACE \t\t _SP\n",
      "I \t\t PRON \t\t PRP\n",
      "hope \t\t VERB \t\t VBP\n",
      "dinner \t\t NOUN \t\t NN\n",
      "'s \t\t PART \t\t POS\n",
      "ready \t\t ADJ \t\t JJ\n",
      "because \t\t SCONJ \t\t IN\n",
      "I \t\t PRON \t\t PRP\n",
      "only \t\t ADV \t\t RB\n",
      "have \t\t VERB \t\t VBP\n",
      "ten \t\t NUM \t\t CD\n",
      "minutes \t\t NOUN \t\t NNS\n",
      "before \t\t SCONJ \t\t IN\n",
      "Mrs. \t\t PROPN \t\t NNP\n",
      "Johnson \t\t PROPN \t\t NNP\n",
      "squirts \t\t VERB \t\t VBZ\n",
      "out \t\t ADP \t\t RP\n",
      "a \t\t DET \t\t DT\n",
      "screamer \t\t NOUN \t\t NN\n",
      ". \t\t PUNCT \t\t .\n",
      "\n",
      " \t\t SPACE \t\t _SP\n",
      "He \t\t PRON \t\t PRP\n",
      "grabs \t\t VERB \t\t VBZ\n",
      "the \t\t DET \t\t DT\n",
      "mail \t\t NOUN \t\t NN\n",
      "and \t\t CCONJ \t\t CC\n",
      "rifles \t\t NOUN \t\t NNS\n",
      "through \t\t ADP \t\t IN\n",
      "it \t\t PRON \t\t PRP\n",
      ", \t\t PUNCT \t\t ,\n",
      "as \t\t SCONJ \t\t IN\n",
      "he \t\t PRON \t\t PRP\n",
      "bends \t\t VERB \t\t VBZ\n",
      "down \t\t ADP \t\t RP\n",
      "to \t\t PART \t\t TO\n",
      "kiss \t\t VERB \t\t VB\n",
      "Sharon \t\t PROPN \t\t NNP\n",
      "on \t\t ADP \t\t IN\n",
      "the \t\t DET \t\t DT\n",
      "cheek \t\t NOUN \t\t NN\n",
      ". \t\t PUNCT \t\t .\n",
      "\n",
      " \t\t SPACE \t\t _SP\n",
      "MICHAEL- \t\t PROPN \t\t NNP\n",
      "C'm \t\t VERB \t\t VBZ\n",
      "on \t\t ADV \t\t RB\n",
      ". \t\t PUNCT \t\t .\n",
      "I \t\t PRON \t\t PRP\n",
      "'m \t\t AUX \t\t VBP\n",
      "supposed \t\t VERB \t\t VBN\n",
      "to \t\t PART \t\t TO\n",
      "give \t\t VERB \t\t VB\n",
      "you \t\t PRON \t\t PRP\n",
      "the \t\t DET \t\t DT\n",
      "tour \t\t NOUN \t\t NN\n",
      ". \t\t PUNCT \t\t .\n",
      "They \t\t PRON \t\t PRP\n",
      "head \t\t VERB \t\t VBP\n",
      "out \t\t ADP \t\t IN\n",
      "of \t\t ADP \t\t IN\n",
      "the \t\t DET \t\t DT\n",
      "office \t\t NOUN \t\t NN\n",
      "\n",
      " \t\t SPACE \t\t _SP\n",
      "MICHAEL \t\t PROPN \t\t NNP\n",
      "( \t\t PUNCT \t\t -LRB-\n",
      "continuing)- \t\t NOUN \t\t NNS\n",
      "So \t\t ADV \t\t RB\n",
      "-- \t\t PUNCT \t\t :\n",
      "which \t\t PRON \t\t WDT\n",
      "Dakota \t\t PROPN \t\t NNP\n",
      "you \t\t PRON \t\t PRP\n",
      "from \t\t ADP \t\t IN\n",
      "? \t\t PUNCT \t\t .\n",
      "\n",
      "          \n",
      "                                 \n",
      " \t\t SPACE \t\t _SP\n",
      "Total tokens: 84\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for word in doc:\n",
    "    print(word.text, '\\t\\t', word.pos_, '\\t\\t', word.tag_)\n",
    "    count += 1\n",
    "print(\"Total tokens:\", count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9cc39d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
